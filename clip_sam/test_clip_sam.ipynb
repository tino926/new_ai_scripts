{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tino926/new_ai_scripts/blob/main/clip_sam/test_clip_sam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6s178XKgicX"
      },
      "source": [
        "The code is primarily copied from https://github.com/maxi-w/CLIP-SAM?tab=readme-ov-file.  \n",
        "The main change made here is to enable the notebook to be run directly in Colab by simply clicking \"Run All.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFROWDl0gUZI"
      },
      "outputs": [],
      "source": [
        "!cd /content\n",
        "\n",
        "import os.path\n",
        "\n",
        "if not os.path.exists(\"CLIP-SAM\"):\n",
        "  !git clone https://github.com/maxi-w/CLIP-SAM.git\n",
        "else:\n",
        "  print(\"CLIP-SAM already exist\")\n",
        "\n",
        "\n",
        "MODEL=\"sam_vit_h_4b8939.pth\"\n",
        "# MODEL=\"sam_vit_l_0b3195.pth\"\n",
        "# MODEL=\"sam_vit_b_01ec64.pth\"\n",
        "if not os.path.exists(MODEL):\n",
        "  !wget https://dl.fbaipublicfiles.com/segment_anything/{MODEL}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HRrMqZpLlmBH"
      },
      "outputs": [],
      "source": [
        "!pip install torch opencv-python Pillow\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iODXUXsxg7Bf"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from segment_anything import build_sam, SamAutomaticMaskGenerator\n",
        "from PIL import Image, ImageDraw\n",
        "import clip\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jClXHwYgm4Lj"
      },
      "outputs": [],
      "source": [
        "# Download the model weights to load them here\n",
        "mask_generator = SamAutomaticMaskGenerator(build_sam(checkpoint=MODEL))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHFBMem_ncRz"
      },
      "outputs": [],
      "source": [
        "image_path = \"CLIP-SAM/assets/example-image.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB9H5GWZngq2"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "masks = mask_generator.generate(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yVkJw9-njoO"
      },
      "outputs": [],
      "source": [
        "def convert_box_xywh_to_xyxy(box):\n",
        "    x1 = box[0]\n",
        "    y1 = box[1]\n",
        "    x2 = box[0] + box[2]\n",
        "    y2 = box[1] + box[3]\n",
        "    return [x1, y1, x2, y2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tm3y6KZnneP"
      },
      "outputs": [],
      "source": [
        "def segment_image(image, segmentation_mask):\n",
        "    image_array = np.array(image)\n",
        "    segmented_image_array = np.zeros_like(image_array)\n",
        "    segmented_image_array[segmentation_mask] = image_array[segmentation_mask]\n",
        "    segmented_image = Image.fromarray(segmented_image_array)\n",
        "    black_image = Image.new(\"RGB\", image.size, (0, 0, 0))\n",
        "    transparency_mask = np.zeros_like(segmentation_mask, dtype=np.uint8)\n",
        "    transparency_mask[segmentation_mask] = 255\n",
        "    transparency_mask_image = Image.fromarray(transparency_mask, mode='L')\n",
        "    black_image.paste(segmented_image, mask=transparency_mask_image)\n",
        "    return black_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUG9eO_ppX0L"
      },
      "outputs": [],
      "source": [
        "# Cut out all masks\n",
        "image = Image.open(image_path)\n",
        "cropped_boxes = []\n",
        "\n",
        "for mask in masks:\n",
        "    cropped_boxes.append(segment_image(image, mask[\"segmentation\"]).crop(convert_box_xywh_to_xyxy(mask[\"bbox\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zlfmic7pZ27"
      },
      "outputs": [],
      "source": [
        "# Load CLIP\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0gzh3AepfBd"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def retriev(elements: list[Image.Image], search_text: str) -> int:\n",
        "    preprocessed_images = [preprocess(image).to(device) for image in elements]\n",
        "    tokenized_text = clip.tokenize([search_text]).to(device)\n",
        "    stacked_images = torch.stack(preprocessed_images)\n",
        "    image_features = model.encode_image(stacked_images)\n",
        "    text_features = model.encode_text(tokenized_text)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    probs = 100. * image_features @ text_features.T\n",
        "    return probs[:, 0].softmax(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1znMMLLHphFk"
      },
      "outputs": [],
      "source": [
        "def get_indices_of_values_above_threshold(values, threshold):\n",
        "    return [i for i, v in enumerate(values) if v > threshold]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbnDG_ChDz3v"
      },
      "outputs": [],
      "source": [
        "for i in range(len(cropped_boxes)):\n",
        "  display(cropped_boxes[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uxy29NqDphst"
      },
      "outputs": [],
      "source": [
        "scores = retriev(cropped_boxes, \"watermelon\")\n",
        "indices = get_indices_of_values_above_threshold(scores, 0.05)\n",
        "\n",
        "segmentation_masks = []\n",
        "\n",
        "for seg_idx in indices:\n",
        "    segmentation_mask_image = Image.fromarray(masks[seg_idx][\"segmentation\"].astype('uint8') * 255)\n",
        "    segmentation_masks.append(segmentation_mask_image)\n",
        "\n",
        "original_image = Image.open(image_path)\n",
        "overlay_image = Image.new('RGBA', image.size, (0, 0, 0, 0))\n",
        "overlay_color = (255, 0, 0, 200)\n",
        "\n",
        "draw = ImageDraw.Draw(overlay_image)\n",
        "for segmentation_mask_image in segmentation_masks:\n",
        "    draw.bitmap((0, 0), segmentation_mask_image, fill=overlay_color)\n",
        "\n",
        "result_image = Image.alpha_composite(original_image.convert('RGBA'), overlay_image)\n",
        "result_image"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOqN0WLYBJssHZNS9tvfysN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}